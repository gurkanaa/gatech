{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "xy=sio.loadmat('./SimulatedDataWithTTF.mat')\n",
    "x=xy['SimulateData']\n",
    "#convert images to 1d array to feed simple RNN\n",
    "image_stream=np.zeros([np.size(x,axis=0),np.size(x,axis=1),x[1,1].size])\n",
    "for i in range(np.size(x,axis=0)):\n",
    "    for j in range(np.size(x,axis=1)):\n",
    "        image_stream[i,j,:]=np.reshape(x[i,j],(x[1,1].size,))\n",
    "train_size=int(np.size(image_stream,axis=0)*0.8)\n",
    "train_set_input=image_stream[:train_size,:,:]\n",
    "test_set_input=image_stream[train_size:,:,:]\n",
    "\n",
    "TTF=xy['TTF1']\n",
    "TTF=np.squeeze(TTF)\n",
    "TTF_train=np.zeros([np.size(train_set_input,axis=0),np.size(train_set_input,axis=1)])\n",
    "TTF_test=np.zeros([np.size(test_set_input,axis=0),np.size(test_set_input,axis=1)])\n",
    "for i in range(np.size(train_set_input,axis=0)):\n",
    "    image_stream_len=int(TTF[i]*0.01)\n",
    "    train_set_input[i,image_stream_len:,:]=0\n",
    "    TTF_train[i,:image_stream_len]=np.fliplr([np.arange(TTF[i]-int(TTF[i]*0.01)*100,TTF[i],100)])\n",
    "\n",
    "for i in range(np.size(test_set_input,axis=0)):\n",
    "    image_stream_len=int(TTF[i+train_size]*0.01)\n",
    "    test_set_input[i,image_stream_len:,:]=0\n",
    "    TTF_test[i,:image_stream_len]=np.fliplr([np.arange(TTF[i+train_size]-int(TTF[i+train_size]*0.01)*100,TTF[i+train_size],100)])\n",
    "\n",
    "TTF_train=TTF_train/8000.0\n",
    "TTF_train[:,-4:]=0\n",
    "TTF_test=TTF_test/8000.0\n",
    "TTF_test[:,-4:]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=np.size(train_set_input,axis=1)\n",
    "input_dim=np.size(train_set_input,axis=2)\n",
    "num_units=10\n",
    "batch_size=4\n",
    "initial_prediction_time=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean absolute percentage error for performance evaluation\n",
    "def mean_absolute_percentage_error(true_val,predicted_val):\n",
    "    count=np.zeros([np.size(true_val,axis=2),1])\n",
    "    percentage_error=np.zeros([np.size(true_val,axis=2),1])\n",
    "    for i in range(np.size(true_val,axis=0)):\n",
    "        for j in range(np.size(true_val,axis=1)):\n",
    "            for k in range(initial_prediction_time,np.size(true_val,axis=2)-4):\n",
    "                if(true_val[i,j,k]!=0.0):\n",
    "                    percentage_error[k]+=(np.abs((true_val[i,j,k]-predicted_val[i,j,k])/true_val[i,j,k]))\n",
    "                    count[k]+=1\n",
    "    for i in range(np.size(true_val,axis=2)):\n",
    "        if count[i]==0:\n",
    "            percentage_error[i]=0\n",
    "        else:\n",
    "            percentage_error[i]=percentage_error[i]/count[i]\n",
    "    return percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs & Outputs => Data\n",
    "encoder_input=tf.placeholder(tf.float32,[batch_size,timestep,input_dim])#input placeholder\n",
    "ground_truth=tf.placeholder(tf.float32,[batch_size,timestep])#true output placeholder\n",
    "#Build encoder part\n",
    "encoder_cell=tf.nn.rnn_cell.BasicLSTMCell(num_units,name='encoder',dtype=tf.float32)#LSTM cell\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "encoder_outputs,encoder_state=tf.nn.dynamic_rnn(encoder_cell,encoder_input,\n",
    "                                                initial_state=initial_state)#Run dynamic RNN\n",
    "W2=tf.Variable(np.random.rand(num_units,timestep),dtype=tf.float32)\n",
    "b2=tf.Variable(np.random.rand(1,1),dtype=tf.float32)\n",
    "#output vector (It is called as vector because outputs are created as vectors of size timestep)\n",
    "W2=tf.Variable(np.random.rand(num_units,1),dtype=tf.float32)\n",
    "b2=tf.Variable(np.random.rand(1,1),dtype=tf.float32)\n",
    "predictions=[tf.matmul(tf.squeeze(encoder_outputs[i,:,:]),W2)+b2 for i in range(encoder_outputs.get_shape().as_list()[0])]\n",
    "predictions=tf.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth=tf.expand_dims(ground_truth,axis=2)\n",
    "mask=tf.sign(tf.abs(ground_truth[:,initial_prediction_time:,:]))\n",
    "error=tf.reduce_sum(((ground_truth[:,initial_prediction_time:,:]-predictions[:,initial_prediction_time:,:])**2)*mask)#error\n",
    "\n",
    "                                                        #AdaGrad is preferred for its convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    epoch_num=5\n",
    "    for i in ['RMS','adam','SGD']:\n",
    "        if i=='RMS':\n",
    "            train_step=tf.train.RMSPropOptimizer(0.001).minimize(error)#optimizer (applies 1 step training)\n",
    "        elif i=='adam':\n",
    "            train_step=tf.train.AdamOptimizer(0.001).minimize(error)#optimizer (applies 1 step training)\n",
    "        else:\n",
    "            train_step=tf.train.GradientDescentOptimizer(0.001).minimize(error)#optimizer (applies 1 step training)\n",
    "        sess.run(tf.global_variables_initializer())#initialize variables\n",
    "        data_length=int(len(TTF_train)/batch_size)\n",
    "        for epoch in range(epoch_num):\n",
    "            sum_error=0.0\n",
    "            max_error=0.0\n",
    "            count_sum=0.0\n",
    "            print(epoch)\n",
    "            for frame_id in range(data_length):\n",
    "                batch_in=train_set_input[frame_id*batch_size:(frame_id+1)*batch_size,:,:]\n",
    "                batch_out=TTF_train[frame_id*batch_size:(frame_id+1)*batch_size,:]\n",
    "                [_error,_train_step,_predictions]=sess.run(\n",
    "                        [error,train_step,predictions],\n",
    "                        feed_dict={\n",
    "                            encoder_input:batch_in,\n",
    "                            ground_truth:np.expand_dims(batch_out,axis=2)\n",
    "                        }\n",
    "                    )\n",
    "                sum_error+=_error\n",
    "                max_error=max(_error,max_error)\n",
    "                count_sum+=1\n",
    "            print(\"Train:\",epoch_num,sum_error/count_sum,max_error)\n",
    "            #Test\n",
    "            \n",
    "            dummy_error=[]\n",
    "            cum_batch_out=[]\n",
    "            cum_predictions=[]\n",
    "            for frame_id in range(int(np.size(TTF_test,axis=0)/batch_size)):\n",
    "                batch_in=test_set_input[frame_id*batch_size:(frame_id+1)*batch_size,:,:]\n",
    "                batch_out=TTF_test[frame_id*batch_size:(frame_id+1)*batch_size,:]\n",
    "                [_error,_mask,_predictions]=sess.run(\n",
    "                        [error,mask,predictions],\n",
    "                        feed_dict={\n",
    "                            encoder_input:batch_in,\n",
    "                            ground_truth:np.expand_dims(batch_out,axis=2)\n",
    "                        }\n",
    "                    )\n",
    "                cum_batch_out.append(batch_out)\n",
    "                cum_predictions.append(_predictions)\n",
    "            cum_predictions=np.squeeze(np.asarray(cum_predictions))\n",
    "            cum_batch_out=np.asarray(cum_batch_out)\n",
    "            mperror=mean_absolute_percentage_error(cum_batch_out,cum_predictions)\n",
    "            plt.figure()\n",
    "            plt.plot(mperror)\n",
    "            plt.show()    \n",
    "            if epoch==epoch_num-1:\n",
    "                np.savez(i+'output.npz', original=cum_batch_out, predicted=cum_predictions)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(gatech)",
   "language": "python",
   "name": "gatech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
