{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "xy=sio.loadmat('./SimulatedDataWithTTF.mat')\n",
    "x=xy['SimulateData']\n",
    "#convert images to 1d array to feed simple RNN\n",
    "image_stream=np.zeros([np.size(x,axis=0),np.size(x,axis=1),x[1,1].size])\n",
    "for i in range(np.size(x,axis=0)):\n",
    "    for j in range(np.size(x,axis=1)):\n",
    "        image_stream[i,j,:]=np.reshape(x[i,j],(x[1,1].size,))\n",
    "train_size=int(np.size(image_stream,axis=0)*0.8)\n",
    "train_set_input=image_stream[:train_size,:,:]\n",
    "test_set_input=image_stream[train_size:,:,:]\n",
    "\n",
    "TTF=xy['TTF1']\n",
    "TTF=np.squeeze(TTF)\n",
    "TTF_train=np.zeros([np.size(train_set_input,axis=0),np.size(train_set_input,axis=1)])\n",
    "TTF_test=np.zeros([np.size(test_set_input,axis=0),np.size(test_set_input,axis=1)])\n",
    "for i in range(np.size(train_set_input,axis=0)):\n",
    "    image_stream_len=int(TTF[i]*0.01)\n",
    "    train_set_input[i,image_stream_len:,:]=0\n",
    "    TTF_train[i,:image_stream_len]=np.fliplr([np.arange(TTF[i]-int(TTF[i]*0.01)*100,TTF[i],100)])\n",
    "\n",
    "for i in range(np.size(test_set_input,axis=0)):\n",
    "    image_stream_len=int(TTF[i+train_size]*0.01)\n",
    "    test_set_input[i,image_stream_len:,:]=0\n",
    "    TTF_test[i,:image_stream_len]=np.fliplr([np.arange(TTF[i+train_size]-int(TTF[i+train_size]*0.01)*100,TTF[i+train_size],100)])\n",
    "\n",
    "TTF_train=TTF_train/8000.0\n",
    "TTF_train[:,-4:]=0\n",
    "TTF_test=TTF_test/8000.0\n",
    "TTF_test[:,-4:]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=np.size(train_set_input,axis=1)\n",
    "input_dim=np.size(train_set_input,axis=2)\n",
    "num_units=10\n",
    "batch_size=4\n",
    "initial_prediction_time=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean absolute percentage error for performance evaluation\n",
    "def mean_absolute_percentage_error(true_val,predicted_val):\n",
    "    count=np.zeros([np.size(true_val,axis=2),1])\n",
    "    percentage_error=np.zeros([np.size(true_val,axis=2),1])\n",
    "    for i in range(np.size(true_val,axis=0)):\n",
    "        for j in range(np.size(true_val,axis=1)):\n",
    "            for k in range(initial_prediction_time,np.size(true_val,axis=2)-4):\n",
    "                if(true_val[i,j,k]!=0.0):\n",
    "                    percentage_error[k]+=(np.abs((true_val[i,j,k]-predicted_val[i,j,k])/true_val[i,j,k]))\n",
    "                    count[k]+=1\n",
    "    for i in range(np.size(true_val,axis=2)):\n",
    "        if count[i]==0:\n",
    "            percentage_error[i]=0\n",
    "        else:\n",
    "            percentage_error[i]=percentage_error[i]/count[i]\n",
    "    return percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs & Outputs => Data\n",
    "encoder_input=tf.placeholder(tf.float32,[batch_size,timestep,input_dim])#input placeholder\n",
    "ground_truth=tf.placeholder(tf.float32,[batch_size,timestep])#true output placeholder\n",
    "#Build encoder part\n",
    "encoder_cell=tf.nn.rnn_cell.BasicLSTMCell(num_units,name='encoder',dtype=tf.float32,activation=tf.nn.sigmoid)#LSTM cell\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "encoder_outputs,encoder_state=tf.nn.dynamic_rnn(encoder_cell,encoder_input,\n",
    "                                                initial_state=initial_state)#Run dynamic RNN\n",
    "W2=tf.Variable(np.random.rand(num_units,timestep),dtype=tf.float32)\n",
    "b2=tf.Variable(np.random.rand(1,1),dtype=tf.float32)\n",
    "#output vector (It is called as vector because outputs are created as vectors of size timestep)\n",
    "W2=tf.Variable(np.random.rand(num_units,1),dtype=tf.float32)\n",
    "b2=tf.Variable(np.random.rand(1,1),dtype=tf.float32)\n",
    "predictions=[tf.matmul(tf.squeeze(encoder_outputs[i,:,:]),W2)+b2 for i in range(encoder_outputs.get_shape().as_list()[0])]\n",
    "predictions=tf.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (4, 80, 1) and (4, 80, 1, 1) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-36d9d29033c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0004\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#optimizer (applies 1 step training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                         \u001b[0;31m#AdaGrad is preferred for its convergence rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(labels, predictions, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return compute_weighted_loss(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \"\"\"\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (4, 80, 1) and (4, 80, 1, 1) are incompatible"
     ]
    }
   ],
   "source": [
    "\n",
    "error=tf.losses.mean_squared_error(ground_truth,predictions)#error\n",
    "train_step=tf.train.GradientDescentOptimizer(0.0004).minimize(error)#optimizer (applies 1 step training)\n",
    "                                                        #AdaGrad is preferred for its convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    epoch_num=5000\n",
    "    mse_track=[]\n",
    "    maxe_track=[]\n",
    "    mp_track=[]\n",
    "    sess.run(tf.global_variables_initializer())#initialize variables\n",
    "    data_length=int(len(TTF_train)/batch_size)\n",
    "    for epoch in range(epoch_num):\n",
    "        sum_error=0.0\n",
    "        max_error=0.0\n",
    "        count_sum=0.0\n",
    "        print(epoch)\n",
    "        for frame_id in range(data_length):\n",
    "            batch_in=train_set_input[frame_id*batch_size:(frame_id+1)*batch_size,:,:]\n",
    "            batch_out=TTF_train[frame_id*batch_size:(frame_id+1)*batch_size,:]\n",
    "            [_error,_train_step,_predictions]=sess.run(\n",
    "                    [error,train_step,predictions],\n",
    "                    feed_dict={\n",
    "                        encoder_input:batch_in,\n",
    "                        ground_truth:np.expand_dims(batch_out,axis=2)\n",
    "                     }\n",
    "                )\n",
    "            sum_error+=_error\n",
    "            max_error=max(_error,max_error)\n",
    "            count_sum+=1\n",
    "        print(\"Train:\",epoch_num,sum_error/count_sum,max_error)\n",
    "        mse_track.append(sum_error/count_sum)\n",
    "        maxe_track.append(max_error)\n",
    "        #Test\n",
    "            \n",
    "        dummy_error=[]\n",
    "        cum_batch_out=[]\n",
    "        cum_predictions=[]\n",
    "        for frame_id in range(int(np.size(TTF_test,axis=0)/batch_size)):\n",
    "            batch_in=test_set_input[frame_id*batch_size:(frame_id+1)*batch_size,:,:]\n",
    "            batch_out=TTF_test[frame_id*batch_size:(frame_id+1)*batch_size,:]\n",
    "            [_error,_predictions]=sess.run(\n",
    "                    [error,predictions],\n",
    "                    feed_dict={\n",
    "                        encoder_input:batch_in,\n",
    "                        ground_truth:np.expand_dims(batch_out,axis=2)\n",
    "                    }\n",
    "                )\n",
    "            cum_batch_out.append(batch_out)\n",
    "            cum_predictions.append(_predictions)\n",
    "        cum_predictions=np.squeeze(np.asarray(cum_predictions))\n",
    "        cum_batch_out=np.asarray(cum_batch_out)\n",
    "        mperror=mean_absolute_percentage_error(cum_batch_out,cum_predictions)\n",
    "        mp_track.append(mperror)\n",
    "        if epoch==epoch_num-1:\n",
    "            np.savez('nowarmsigmoid.npz', original=cum_batch_out, predicted=cum_predictions, \n",
    "                     mse=np.asarray(mse_track), maxe=np.asarray(maxe_track), mp=np.asarray(mp_track))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
